---
title: "Repeated Cross Validation in Logistic Regression for predicting Gender of Palmer Penguins of Antarctica"
author: "FRK"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    highlight: tango
    toc: TRUE
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 6, eval = T)
```

# Reference

This is an illustration of the work by [Julia Silge](https://juliasilge.com/blog/palmer-penguins/) and [Allison Horst](https://allisonhorst.github.io/palmerpenguins/articles/intro.html).  

Here I used a Repeated k-fold Cross validation to see if there is any improvement or difference in performance of the model in repeated folds.

***

```{r, echo=FALSE, fig.cap="Palmer Penguins", fig.show='hold', fig.align='center', eval=TRUE}
knitr::include_graphics(c("cover.png"))
``` 

***

# Cross validation and Repetition

In a k-fold cross validation, the data is Randomly split at 5 folds, if k=5. Then we iteratively fit model in 4 **train** folds, and test the model on the **test** fold. We do this 5 times and record the performance of the model. Then we take average of performance. This way we can get a robust estimate of the performance.

Now, the data is split randomly at first, we don't know if this split results in maximum randomness. So to get even more robust / accurate estimate we **repeat** this 5-fold validation several times, and take average over all repetition

***

```{r, echo=FALSE, out.width="29%",  out.height="10%", fig.cap="5-fold Cross Validation", fig.show='hold', fig.align='center', eval=TRUE}
knitr::include_graphics(c("image-86 - Copy.png"))
``` 

***

```{r, echo=FALSE, out.width="29%",  out.height="8%", fig.cap="Repeated 5-fold Cross Validation", fig.show='hold', fig.align='center', eval=TRUE}
knitr::include_graphics(c("image-86 - Copy.png", 
                          "image-86 - Copy.png",
                          "image-86 - Copy.png",
                          "image-86 - Copy.png",
                          "image-86 - Copy.png"))
``` 

***

# Loading Packages

```{r}
if (!require('palmerpenguins')) devtools::install_github("allisonhorst/palmerpenguins"); library('palmerpenguins')

require(ggplot2)
require(tidymodels)

```

# Data

```{r}

penguins

glimpse(penguins)

```

Today we will build a logistic regression model to predict the *Gender* of the palmer penguin, using body part lengths and body mass data.

# Plot the Data

Plot the data by *flipper length* and *bill length* of the penguins, also use *body mass* in the bubble plot to see for any relationship with the *gender* of the penguins. 

```{r}
penguins %>%
  filter(!is.na(sex)) %>%
  ggplot(aes(flipper_length_mm, bill_length_mm, color = sex, size = body_mass_g)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~species)

```



```{r}
penguins %>%
  drop_na() %>% 
  select(species, body_mass_g, ends_with("_mm"), sex) %>%
  GGally::ggpairs(aes(color = sex, alpha = 0.5))
```


Looks like there is relation between sex, body part lengths and body mass of the penguins.  

***

```{r, echo=FALSE, fig.cap="Body Parts of Penguins", fig.show='hold', fig.align='center', eval=TRUE}
knitr::include_graphics(c("download.jpg"))
``` 

***


***

# Modelling Logistic Regression

## Data for modelling

We drop *year* and *island* from our data. Also drop missing observations in *sex*

```{r}

penguins_df <- penguins %>%
  filter(!is.na(sex)) %>%
  select(-year, -island)
penguins_df

levels(penguins_df$sex)

```

## Splitting Into Training and Testing Data

We use **tidymodels** package for modelling *gender* of the palmer penguins. 

```{r}

set.seed(123)
penguin_split <- initial_split(penguins_df, strata = sex)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
penguin_split
```

## Create Repeated 10-fold Cross Validation Dataset

```{r}

penguin_cv <- vfold_cv(data = penguin_train, v = 10, repeats = 10, strata = sex)
penguin_cv

```

## Specify Model

```{r}

glm_spec <- logistic_reg() %>%
  set_engine("glm")

```

## Specify Workflow

```{r}
penguin_wf <- workflow() %>%
  add_formula(sex ~ .)

```

## Fit the Logistic Model in CV datasets

*fit_resamples* fits the logistic model in each of the 100 training datasets in the *penguin_cv* set, and evaluates the model on each of the 100 testing datasets. It also saves the predictions for evaluating performance of the model on each dataset.

```{r}

### Parallel Processing makes things faster
### tidymodels support parallel processing

doParallel::registerDoParallel()

glm_rs <- penguin_wf %>%
  add_model(glm_spec) %>%
  fit_resamples(
    resamples = penguin_cv,
    control = control_resamples(save_pred = TRUE, verbose = TRUE)
  )

glm_rs

```

## Check Model Accuracy

This accuracy and AUC is mean over all CV dataset.

```{r}

collect_metrics(glm_rs)

glm_rs %>% 
  unnest(.metrics) %>% 
  ggplot(aes(id2, .estimate, color = .metric)) + 
  geom_point() +
  labs(title = "Accurary and ACU over Folds and Repetitions",
       x = "Fold", 
       y = NULL,
       color = "Metric") +
  facet_wrap(.metric ~ id) +
  theme(axis.text.x = element_text(size=6, angle = 90)) 
  

```

## Confusion Matrix

Also showing average numbers in the confusion matrix.

```{r}

glm_rs %>%
  conf_mat_resampled()
```

## ROC curves over folds and repeats

The ROC curve shows similar performance over repeats, although some variation is seen over the folds within repeats.

```{r}
glm_rs %>%
  collect_predictions() %>%
  group_by(id, id2) %>%
  roc_curve(sex, .pred_female) %>%
  ggplot(aes(1 - specificity, sensitivity, color = id2)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_path(show.legend = TRUE, alpha = 0.5, size = 0.8) +
  coord_equal() +
  facet_wrap(~id) + 
  labs(color='Fold', x = "1 - Specificity", y = "Sensitivity", title = "ROC & AUC by Fold and Repeat") +
  theme_minimal()

```



## Finalize model on Whole Training Data and Test on Testing data


```{r}

penguin_final <- penguin_wf %>%
  add_model(glm_spec) %>%
  last_fit(penguin_split)

penguin_final

```

## Metrics over Test Data

The metrics on testing data shows similar performance with the CV data. This indicated absence of overfitting, and good predictive performance of the logistic model for new data.

```{r}

collect_metrics(penguin_final)

collect_predictions(penguin_final) %>%
  conf_mat(sex, .pred_class)

collect_predictions(penguin_final) %>%
  sensitivity(sex, .pred_class)

collect_predictions(penguin_final) %>%
  specificity(sex, .pred_class)

collect_predictions(penguin_final) %>%
  precision(sex, .pred_class)
```

## Odds and Variables

Looks like *bill depth* and *bill length* have highest importance in predicting *gender* of the penguins. These two variables separate the penguins by *gender* most.

```{r}
penguin_final$.workflow[[1]] %>%
  tidy(exponentiate = TRUE)

penguin_final$.workflow[[1]] %>%
  tidy(exponentiate = TRUE) %>% 
  select(term, estimate) %>% 
  mutate(term = as.factor(term)) %>% 
  ggplot(aes(reorder(term, estimate), estimate, 
             fill =term)) +
  geom_bar(stat = "identity", show.legend = FALSE, width = 0.7) + 
  labs(title = "Increase in odds of penguin being Female by one unit increase in each variable",
       x = "Variable", y = "Odds increase by Times") +
  geom_text(aes(label = round(estimate, 3)), 
            nudge_y = 0.15 , 
            size = 4.5, 
            colour = 'black', 
            fontface = 'bold') +
  coord_flip() +
  theme_light()

```


1mm increase in *bill depth* increases the odds of the penguin being female by almost **4 times**.

***

# Bill Depth vs Bill Length Scatter

```{r}
  
penguins %>%
  filter(!is.na(sex)) %>%
  ggplot(aes(bill_depth_mm, bill_length_mm, color = sex, size = body_mass_g)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~species)

```

***

```{r, echo=FALSE, fig.cap="Bill Length and Bill Depth", fig.show='hold', fig.align='center', eval=TRUE}
knitr::include_graphics(c("culmen_depth.png"))
``` 

***
